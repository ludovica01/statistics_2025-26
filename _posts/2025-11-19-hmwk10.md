---
layout: post
title: "Probability Theory: Interpretations, Axioms, and Measure-Theoretic Foundations"
date: 2025-11-26 14:30:00 +01:00
author: Ludovica
---

<h2>Interpretations of Probability and the Axiomatic Framework</h2>

<p>
Probability is a fundamental concept in statistics and can be interpreted in different ways depending on the philosophical or practical context. The most important interpretations are the classical, frequentist, Bayesian, and geometric approaches. Although they appear conceptually different, they are all unified and made mathematically consistent by the axiomatic framework introduced by Kolmogorov.
</p>

<h3>Classical Interpretation</h3>

<p>
In the classical interpretation, probability is defined as the ratio between the number of favorable outcomes and the total number of equally likely outcomes. For example, when rolling a fair six-sided die, the probability of obtaining a 4 is equal to 1/6 because only one outcome is favorable out of six possible outcomes.
</p>

<p>
This interpretation works well for simple symmetric experiments, but it becomes problematic when the notion of “equally likely” outcomes is unclear or impossible to define rigorously.
</p>

<h3>Frequentist Interpretation</h3>

<p>
According to the frequentist view, probability is the limit of the relative frequency of an event in a long sequence of repeated independent trials. If an event A occurs Xₙ times in n trials, its probability is defined as:
</p>

<p>
P(A) = lim (n → ∞) Xₙ / n
</p>

<p>
This interpretation is strongly linked to real data and experimental observations, but it cannot naturally handle one-time events, such as forecasting tomorrow’s weather.
</p>

<h3>Bayesian Interpretation</h3>

<p>
In the Bayesian framework, probability represents a degree of belief rather than a physical frequency. Information is updated using Bayes’ theorem:
</p>

<p>
P(A | B) = (P(B | A) · P(A)) / P(B)
</p>

<p>
The initial belief is modeled by a prior distribution, which is updated into a posterior distribution after observing new data. This interpretation is very powerful for decision-making and learning from limited data, but it depends on the subjective choice of the prior.
</p>

<h3>Geometric Interpretation</h3>

<p>
Geometric probability is applied when the sample space is continuous and events correspond to geometric regions. The probability of an event is defined as the ratio between the measure (length, area, volume) of the favorable region and the total measure of the sample space.
</p>

<p>
For example, if a point is chosen uniformly at random inside a unit square, the probability that it lies inside a circle of radius r is equal to the area of the circle divided by the area of the square.
</p>

<h3>The Role of the Axiomatic Approach</h3>

<p>
The different interpretations may appear contradictory, but Kolmogorov’s axiomatic framework resolves these issues by defining probability through three fundamental axioms:
</p>

<ul>
  <li>Non-negativity: P(A) ≥ 0 for every event A.</li>
  <li>Normalization: P(Ω) = 1.</li>
  <li>Countable additivity: for pairwise disjoint events Aᵢ, P(∪Aᵢ) = ΣP(Aᵢ).</li>
</ul>

<p>
These axioms define probability as a mathematical measure, independent of any philosophical interpretation. Classical, frequentist, Bayesian, and geometric probabilities are all valid probability measures within this framework. As a result, the axiomatic approach removes conceptual inconsistencies and provides a single rigorous foundation for all interpretations.
</p>

<h2>Probability Theory and Measure Theory</h2>

<p>
Modern probability theory is rigorously formulated using measure theory. A probability space is defined as a triple (Ω, F, P), where Ω is the sample space, F is a sigma-algebra, and P is a probability measure.
</p>

<h3>Sigma-Algebras</h3>

<p>
A sigma-algebra F is a collection of subsets of Ω that satisfies three properties: it contains Ω itself, it is closed under complements, and it is closed under countable unions. Sigma-algebras ensure that probabilities can be assigned consistently to events.
</p>

<h3>Probability Measures</h3>

<p>
A probability measure P assigns a number between 0 and 1 to each event in F and satisfies the three axioms of probability. From a measure-theoretic perspective, probability is simply a normalized measure.
</p>

<h3>Random Variables as Measurable Functions</h3>

<p>
A random variable is defined as a measurable function X: Ω → R. Measurability guarantees that events of the form {X ≤ x} are contained in the sigma-algebra F, allowing probabilities to be assigned.
</p>

<p>
The expected value of a random variable is defined as a Lebesgue integral:
</p>

<p>
E[X] = ∫Ω X(ω) dP(ω)
</p>

<p>
This formulation unifies discrete sums and continuous integrals into a single mathematical object.
</p>

<h2>Derivation of Subadditivity</h2>

<p>
Using only the axioms of probability, it is possible to derive several important properties. One of them is subadditivity, which states that for any sequence of events A₁, A₂, A₃, … :
</p>

<p>
P(∪Aᵢ) ≤ ΣP(Aᵢ)
</p>

<p>
To prove this, each event Aᵢ can be decomposed into disjoint parts by removing the portions that overlap with previous events. Since probability is monotone and countably additive for disjoint events, the inequality immediately follows.
</p>

<h2>Inclusion–Exclusion Principle</h2>

<p>
For two events A and B, the inclusion–exclusion principle states:
</p>

<p>
P(A ∪ B) = P(A) + P(B) − P(A ∩ B)
</p>

<p>
This formula corrects the double counting of the intersection A ∩ B. It is derived by expressing B as the union of the disjoint sets (B \ A) and (A ∩ B).
</p>

<p>
For three events A, B, and C, the formula becomes:
</p>

<p>
P(A ∪ B ∪ C) = P(A) + P(B) + P(C) − P(A ∩ B) − P(A ∩ C) − P(B ∩ C) + P(A ∩ B ∩ C)
</p>

<p>
In general, for n events, the inclusion–exclusion principle alternates sums and differences of higher-order intersections. This principle is fundamental in combinatorics and probability, especially when dealing with overlapping events.
</p>

<h2>Practical Examples with Code</h2>

<h3>Frequentist Simulation of a Biased Coin</h3>

<img src="{{ site.baseurl }}/assets/images/hmwk10-grandi-numeri.png" alt="Grandi numeri" class="center-image">

<img src="{{ site.baseurl }}/assets/images/hmwk10-screen-dati.png" alt="Risultati grandi numeri" class="center-image">

<p>
The following Python code simulates repeated flips of a biased coin with probability p = 0.6 and shows how the empirical frequency converges to the true probability.
</p>


import numpy as np

p_true = 0.6
n_trials = 10000

rng = np.random.default_rng(0)
flips = rng.random(n_trials) < p_true

empirical_frequency = np.cumsum(flips) / np.arange(1, n_trials + 1)

print(empirical_frequency[-1])

<img src="{{ site.baseurl }}/assets/images/hmwk10-bayesian-curve.png" alt="Bayesian Curve" class="center-image">


<p>
The figure illustrates how prior uncertainty about the parameter p is updated after observing experimental data. The posterior distribution becomes more concentrated around the most plausible values supported by the observations.
</p>


<p> This experiment illustrates the frequentist interpretation: the empirical relative frequency converges to the true probability as the number of trials increases. </p>


<h3>Bayesian Update with a Beta Prior</h3>
<p>
In the Bayesian framework, probability is interpreted as a degree of belief about an unknown parameter and is updated as new data become available. For binomial experiments, a common choice for the prior distribution is the Beta distribution, since it is defined on the interval [0,1] and is conjugate to the binomial likelihood.
</p>

<p>
Starting from a prior Beta(α, β), and observing k successes over n trials, the posterior distribution is again a Beta distribution with updated parameters α + k and β + n − k. This analytical form makes Bayesian updating mathematically simple and computationally efficient. The resulting posterior summarizes both prior information and observed data in a single probability model.
</p>

<p> Consider n = 20 coin flips with k = 14 heads. Using a Beta(1,1) prior and a binomial likelihood, the posterior distribution is Beta(15,7). The following code computes the posterior mean: </p>


alpha_post = 15
beta_post = 7

posterior_mean = alpha_post / (alpha_post + beta_post)
print(posterior_mean)

<p> This example shows how Bayesian probability updates beliefs using data, while remaining fully consistent with the axiomatic framework. </p>