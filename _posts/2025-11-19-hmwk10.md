---
layout: post
title: "Probability Theory: Interpretations, Axioms, and Measure-Theoretic Foundations"
date: 2025-11-26 14:30:00 +01:00
author: Ludovica
---

<h2><strong>Interpretations of Probability and the Axiomatic Framework</strong></h2>

<p>
Probability is a fundamental concept in statistics and can be interpreted in different ways depending on the philosophical or practical context. The most important interpretations are the classical, frequentist, Bayesian, and geometric approaches. Although they appear conceptually different, they are all unified and made mathematically consistent by the axiomatic framework introduced by Kolmogorov.
</p>

<h3>Classical Interpretation</h3>

<p>
In the classical interpretation, probability is defined as the ratio between the number of favorable outcomes and the total number of equally likely outcomes. For example, when rolling a fair six-sided die, the probability of obtaining a 4 is equal to 1/6 because only one outcome is favorable out of six possible outcomes.
</p>

<p>
This interpretation works well for simple <strong>symmetric experiments</strong>, but it becomes problematic when the notion of “equally likely” outcomes is unclear or impossible to define rigorously.
</p>

<h3>Frequentist Interpretation</h3>

<p>
According to the frequentist view, probability is the limit of the relative frequency of an event in a long sequence of repeated independent trials. If an event A occurs Xₙ times in n trials, its probability is defined as:
</p>

<p>
P(A) = lim (n → ∞) Xₙ / n
</p>

<p>
This interpretation is strongly linked to real data and experimental observations, but it cannot naturally handle one-time events, such as forecasting tomorrow’s weather.
</p>

<h3>Bayesian Interpretation</h3>

<p>
In the Bayesian framework, probability represents a degree of belief rather than a physical frequency. Information is updated using Bayes’ theorem:
</p>

<p>
P(A | B) = (P(B | A) · P(A)) / P(B)
</p>

<p>
The initial belief is modeled by a prior distribution, which is updated into a posterior distribution after observing new data. This interpretation is very powerful for decision-making and learning from limited data, but it depends on the subjective choice of the prior.
</p>

<h3>Geometric Interpretation</h3>

<p>
Geometric probability is applied when the sample space is continuous and events correspond to geometric regions. The probability of an event is defined as the ratio between the measure (length, area, volume) of the favorable region and the total measure of the sample space.
</p>

<p>
For example, if a point is chosen uniformly at random inside a unit square, the probability that it lies inside a circle of radius r is equal to the area of the circle divided by the area of the square.
</p>

<h3>The Role of the Axiomatic Approach</h3>

<p>
The different interpretations may appear contradictory, but Kolmogorov’s axiomatic framework resolves these issues by defining probability through three fundamental axioms:
</p>

<ul>
  <li><strong>Non-negativity</strong>: P(A) ≥ 0 for every event A.</li>
  <li><strong>Normalization</strong>: P(Ω) = 1.</li>
  <li><strong>Countable additivity</strong>: for pairwise disjoint events Aᵢ, P(∪Aᵢ) = ΣP(Aᵢ).</li>
</ul>

<p>
These axioms define probability as a mathematical measure, independent of any philosophical interpretation. Classical, frequentist, Bayesian, and geometric probabilities are all valid probability measures within this framework. As a result, the axiomatic approach removes conceptual inconsistencies and provides a single rigorous foundation for all interpretations.
</p>

<h2>Probability Theory and Measure Theory</h2>

<p>
Modern probability theory is rigorously formulated using measure theory. A probability space is defined as a triple <em>(Ω, F, P)</em>, where Ω is the sample space, <em>F</em> is a sigma-algebra, and <em>P</em> is a probability measure.
</p>

<h3>Sigma-Algebras</h3>

<p>
A sigma-algebra <em>F</em> is a collection of subsets of Ω that satisfies three properties: it contains Ω itself, it is closed under complements, and it is closed under countable unions. Sigma-algebras ensure that probabilities can be assigned consistently to events.
</p>

<h3>Probability Measures</h3>

<p>
A probability measure <em>P</em> assigns a number between 0 and 1 to each event in <em>F</em> and satisfies the three axioms of probability. From a measure-theoretic perspective, probability is simply a normalized measure.
</p>

<h3>Random Variables as Measurable Functions</h3>

<p>
A random variable is defined as a measurable function <em>X</em>: Ω → <em>R</em>. Measurability guarantees that events of the form <em>{X ≤ x}</em> are contained in the sigma-algebra <em>F</em>, allowing probabilities to be assigned.
</p>

<p>
The expected value of a random variable is defined as a <strong>Lebesgue integral</strong>:
</p>

<p>
E[X] = ∫Ω X(ω) dP(ω)
</p>

<p>
This formulation unifies discrete sums and continuous integrals into a single mathematical object.
</p>

<h2>Derivation of Subadditivity</h2>

<p>
Using only the axioms of probability, it is possible to derive several important properties. One of them is subadditivity, which states that for any sequence of events A₁, A₂, A₃, … :
</p>

<p>
P(∪Aᵢ) ≤ ΣP(Aᵢ)
</p>

<p>
To prove this, each event Aᵢ can be decomposed into disjoint parts by removing the portions that overlap with previous events. Since probability is monotone and countably additive for disjoint events, the inequality immediately follows.
</p>

<h2>Inclusion–Exclusion Principle</h2>

<p>
For two events A and B, the <strong>inclusion–exclusion</strong> principle states:
</p>

<p>
P(A ∪ B) = P(A) + P(B) − P(A ∩ B)
</p>

<p>
This formula corrects the double counting of the intersection A ∩ B. It is derived by expressing B as the union of the disjoint sets (B \ A) and (A ∩ B).
</p>

<p>
For three events A, B, and C, the formula becomes:
</p>

<p>
P(A ∪ B ∪ C) = P(A) + P(B) + P(C) − P(A ∩ B) − P(A ∩ C) − P(B ∩ C) + P(A ∩ B ∩ C)
</p>

<p>
In general, for n events, the inclusion–exclusion principle alternates sums and differences of higher-order intersections. This principle is fundamental in combinatorics and probability, especially when dealing with overlapping events.
</p>


<h3>Frequentist Simulation of a Biased Coin</h3>

<img src="{{ site.baseurl }}/assets/images/hmwk10-grandi-numeri.png" alt="Grandi numeri" class="center-image">

<img src="{{ site.baseurl }}/assets/images/hmwk10-screen-dati.png" alt="Risultati grandi numeri" class="center-image">

<p>


<p>
The figure illustrates how prior uncertainty about the parameter p is updated after observing experimental data. The posterior distribution becomes more concentrated around the most plausible values supported by the observations.
</p>


<p> This experiment illustrates the frequentist interpretation: the empirical relative frequency converges to the true probability as the number of trials increases. </p>


<h3>Bayesian Update with a Beta Prior</h3>
<p>
In the Bayesian framework, probability is interpreted as a degree of belief about an unknown parameter and is updated as new data become available. For binomial experiments, a common choice for the prior distribution is the Beta distribution, since it is defined on the interval [0,1] and is conjugate to the binomial likelihood.
</p>

<p>
Starting from a prior <em>Beta(α, β)</em>, and observing k successes over n trials, the posterior distribution is again a Beta distribution with updated parameters <em>α + k</em> and <em>β + n − k</em>. This analytical form makes Bayesian updating mathematically simple and computationally efficient. The resulting posterior summarizes both prior information and observed data in a single probability model.
</p>

<p> Consider <em>n</em> = 20 coin flips with <em>k</em> = 14 heads. Using a <em>Beta(1,1)</em> prior and a binomial likelihood, the posterior distribution is Beta(15,7). The following code computes the posterior mean: </p>


alpha_post = 15
beta_post = 7

posterior_mean = alpha_post / (alpha_post + beta_post)
print(posterior_mean)

<p> This example shows how Bayesian probability updates beliefs using data, while remaining fully consistent with the axiomatic framework. </p>


<img src="{{ site.baseurl }}/assets/images/hmwk10-bayesian-curve.png" alt="Bayesian Curve" class="center-image">












<h2>Simulating a Counting Process</h2>

<p>In this section, we simulate a <strong>counting process</strong> over a fixed time interval <em>T</em>, where events (“successes”) occur <strong>independently and uniformly in time</strong> at a <strong>constant average rate</strong> <em>λ</em>. This is a typical example of a <strong>Poisson process</strong>, one of the most fundamental stochastic processes in probability theory and statistics.</p>

<h3>Step 1: Discretize the Time Interval</h3>

<p>To simulate the process, we divide the interval <em>T</em> into <em>n</em> small subintervals of equal length <em>Δt = T / n</em>. In each subinterval, we generate an event with probability <em>p = λ / n</em>. This approximation is valid because, for a small enough <em>Δt</em>, the probability of more than one event in a subinterval is negligible.</p>

<h3>Step 2: Generate the Counting Process</h3>

<p>After generating random events in each subinterval and counting them cumulatively, we obtain a step function, where each step corresponds to the occurrence of an event. A plot of this process would show a staircase-like graph, visually representing the cumulative number of events over time.</p>

<h3>Step 3: Analyze the Process</h3>

<p>The simulated process approximates a <strong>Poisson process</strong> with rate <em>λ</em>. Key properties of this process include:</p>

<ul>
  <li><strong>Independent increments</strong>: the number of events in disjoint time intervals is independent.</li>
  <li><strong>Stationary increments</strong>: the probability of a given number of events depends only on the length of the interval, not its position.</li>
  <li><strong>Memoryless property</strong>: the waiting times between consecutive events are <em>exponentially distributed</em> with mean 1/λ.</li>
</ul>

<p>The number of events <em>N(t)</em> in any interval of length <em>t</em> follows a <strong>Poisson distribution</strong>:</p>

<p style="text-align:center;"><em>P(N(t) = k) = (λ t)^k e^(-λ t) / k!, for k = 0,1,2,...</em></p>

<h3>Step 4: Interpretation of λ</h3>

<p>The parameter <em>λ</em> represents the <strong>average rate of events per unit time</strong>. For example, if λ = 5, we expect, on average, 5 events to occur per unit of time T. Changing λ changes the frequency of events: doubling λ approximately doubles the number of steps in the cumulative count plot.</p>

<h3>Step 5: Example Interpretation</h3>

<p>Suppose λ = 5 and T = 1. After simulating the process, the final count might be around 5 events (it can vary due to randomness). The steps in the plot correspond to the moments when an event occurs. Increasing λ to 10 would produce roughly twice as many steps in the same interval, illustrating how the rate parameter affects the dynamics.</p>


<img src="{{ site.baseurl }}/assets/images/hmwk10-simulation.png" alt="Bayesian Curve" class="center-image">

<p>The image is included to visually illustrate the counting process. It clearly shows how events accumulate over time and how the rate parameter λ affects the number of occurrences, without overloading the text with programming details.</p>
