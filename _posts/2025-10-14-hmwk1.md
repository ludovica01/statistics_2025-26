---
layout: post
title: "First Statistics Homework"
date: 2025-10-14
author: Ludovica
---

<h1>Statistical Distributions and Datasets</h1>

In statistics, a dataset is a structured collection of data, typically organized in a tabular form, where each row represents an observation (or statistical unit) and each column represents a variable describing a specific attribute of those observations. Variables can be quantitative (numerical) or qualitative (categorical), and their measurement scales—nominal, ordinal, interval, or ratio—determine the types of analysis that can be performed. Datasets are the foundation of statistical investigation, as they provide the raw information from which patterns, relationships, and inferences can be drawn. A well-constructed dataset should be accurate, complete, and representative of the population under study.

A statistical distribution describes how the values of a variable are spread or distributed within a dataset. It shows the frequency or probability of different outcomes and helps to understand the overall behavior of the data. For example, a frequency distribution lists the number of times each value occurs, while a probability distribution assigns probabilities to each possible value of a random variable. Distributions can be discrete, when the variable takes distinct values (such as the number of students in a class), or continuous, when the variable can assume any value within an interval (such as height or temperature).

Understanding the shape of a distribution—whether it is symmetric, skewed, or has multiple peaks—is essential for statistical analysis. Common theoretical models, like the normal distribution, binomial distribution, and Poisson distribution, provide mathematical frameworks to describe real-world phenomena. These models allow researchers to make predictions, estimate parameters, and test hypotheses. In essence, datasets provide the empirical basis of statistical work, while distributions offer the conceptual tools to interpret and generalize from the data.


<h2>Case Study Description: Analysis of Preferences on Streaming Platforms</h2>

<h3>Objective of the Analysis</h3>
<p>The goal of this project is to analyze a small data sample to describe user preferences across different streaming platforms. Using a custom-created dataset, the analysis aims to explore how genre preferences and service ratings are distributed among the main platforms.</p>

<h3>Dataset Context and Attributes</h3>
<p>We have simulated data collection from 15 users, recording three key pieces of information for each. These three attributes (or statistical variables) form the columns of our dataset:</p>

<p><img src="{{ site.baseurl }}//assets/images/streaming_preferences-1.png" alt="Here is our dataset"></p>

<ul>
  <li><strong>Platform:</strong> Indicates the streaming service used by the user.
    <ul>
      <li>Type: Qualitative Variable.</li>
      <li>Scale of Measurement: Nominal, as the categories ('Netflix', 'Prime Video', 'Disney+') have no intrinsic order.</li>
    </ul>
  </li>

  <li><strong>Preferred Genre:</strong> Represents the user's favorite content genre.
    <ul>
      <li>Type: Qualitative Variable.</li>
      <li>Scale of Measurement: Nominal, since the genres ('Action', 'Comedy', 'Drama') do not follow a predefined order.</li>
    </ul>
  </li>

  <li><strong>Service Rating:</strong> Expresses the user's judgment of the platform's quality.
    <ul>
      <li>Type: Qualitative Variable.</li>
      <li>Scale of Measurement: Ordinal, because the modalities ('Low', 'Medium', 'High') represent a hierarchical order of satisfaction.</li>
    </ul>
  </li>
</ul>

<p>The set of 15 records forms our data matrix, where each row represents a statistical unit (the user).</p>

<p>The work was carried out following a precise methodological procedure, using a database management system (DBMS) to ensure data integrity and analysis.</p>

<ul>
  <li><strong>Database Setup:</strong> PostgreSQL was used as the DBMS. A table named <code>streaming_preferences</code> was created to host the dataset. The table structure was defined using the <code>CREATE TABLE</code> command, specifying the names and data types for the three attributes and a primary key (<code>id</code>).</li>

  <li><strong>Data Insertion:</strong> The table was populated with 15 sample records using the <code>INSERT INTO</code> command. This step transformed the empty structure into a concrete and analyzable statistical dataset.</li>

  <li><strong>Frequency Distribution Calculation:</strong> To meet the exercise's requirement, the trivariate distribution (or k-variate, with k=3) of absolute frequencies was calculated. An SQL query was executed with the following logic:
    <ol>
      <li><code>SELECT</code>: To specify the three columns of interest (<code>platform</code>, <code>preferred_genre</code>, <code>service_rating</code>).</li>
      <li><code>COUNT(*)</code>: To count the occurrences of each group.</li>
      <li><code>GROUP BY</code>: To group the rows that share the same unique combination of the three attributes.</li>
    </ol>
  </li>

  <li><strong>Results Visualization:</strong> The query produced a joint frequency table as its result, which shows exactly how many times each combination (e.g., "a Netflix user who prefers the Drama genre and rates the service as High") appears in our data sample. This final output represents the main result of the descriptive analysis.</li>
</ul>




![And here is its distribution]({{ site.baseurl }}//assets/images/streaming_preferences-2.png)


<h3>Bivalent analysis: Platform vs. Service Rating</h3>
To further explore the dataset, a bivariate analysis was performed, focusing on the relationship between two specific variables: Platform and Service Rating.

The objective was to move from a three-dimensional view to a two-dimensional one, in order to observe the joint distribution of these two attributes more clearly. An SQL query was executed to group the 15 records by platform and its corresponding service rating, counting the absolute frequency for each pair.

The result is a contingency table, which cross-tabulates the two variables. This table allows for a direct comparison of user satisfaction levels across the different streaming services in our sample, revealing preliminary patterns and associations.


![bivalent distribution]({{ site.baseurl }}//assets/images/streaming_preferences-3.png)



<h2>Cryptanalysis of a Caesar Cipher Using Frequency Analysis</h2>

<h3>Objective</h3>
This exercise demonstrates a foundational technique in cryptanalysis: <strong>frequency analysis</strong>. The goal is to encrypt a sample text using a simple substitution cipher (the Caesar cipher) and then break the encryption without prior knowledge of the key. This process highlights how statistical properties of a language can be used to defeat ciphers.

<h3>Methodology and Steps</h3>
The process was divided into three main stages:

<ol>
  <li>
    <strong>Establishing a Baseline: Original Text Analysis</strong>
    First, we analyzed the letter frequency of the original source text—the first 12 articles of the Italian Constitution. A Python script was executed to iterate through the text, count the occurrence of each letter, and calculate its relative frequency. This initial step is crucial as it creates a unique statistical "fingerprint" of the plaintext. As expected for Italian text, the results showed a high frequency for vowels like <em>e</em> and <em>a</em>, establishing a clear, predictable pattern.

    ![statistics on the chosen text]({{ site.baseurl }}//assets/images/analisi_alfabeto1.png)


  </li>

  <li>
    <strong>Encryption with the Caesar Cipher</strong>
    Next, the original text was encrypted using the Caesar cipher with a predefined shift key of 5. This process algorithmically substituted each letter with the one found five positions further down the alphabet (A → F, B → G, etc.), wrapping around from Z back to A. The output was an unreadable ciphertext. At this stage, the message appears secure to a casual observer, as the direct meaning is completely obscured.
    
    
    
    ![example of plain text]({{ site.baseurl }}//assets/images/analisi_alfabeto3.png)
    
    
    ![example of cyphered text]({{ site.baseurl }}//assets/images/analisi_alfabeto2.png)


  </li>


<li>
        here you can see the distribution of each character into the cypher text analyzed.
        
        ![graph cypher text]({{ site.baseurl }}//assets/images/analisi_alfabeto(grafico).png)

</li>

  <li>
    <strong>Cryptanalysis Through Frequency Analysis</strong>
    This is the core of the exercise. Assuming the role of an analyst with access only to the ciphertext, we proceeded as follows:
    <ul>
      <li><strong>Frequency Analysis of Ciphertext:</strong> The same Python script was run on the encrypted text, generating a new frequency distribution table for the cipher's characters.</li>
      <li><strong>Comparison and Hypothesis:</strong> The ciphertext's letter distribution was compared against a standard frequency chart for the Italian language. The central hypothesis is that the most frequent letter in the ciphertext corresponds to the most frequent letter in standard Italian. For example, since <em>e</em> is the most common letter in Italian, we looked for the most common letter in our ciphertext (which, with a key of 5, turned out to be <em>j</em>).</li>
      <li><strong>Deducing the Key:</strong> By calculating the positional shift between the hypothesized pair (the distance from <em>e</em> to <em>j</em> is 5), we successfully deduced the secret key. This hypothesis was confirmed by applying the inverse shift (-5) to a segment of the ciphertext, which immediately revealed readable Italian words.</li>
    </ul>
  </li>
</ol>

<h3>Conclusion: A Powerful Analytical Tool </h3>
This exercise demonstrates that even when a message is encrypted, it often retains a statistical shadow of its original language.

<strong>Frequency analysis</strong> is a powerful tool because it exploits this fundamental weakness, which is present in all simple substitution ciphers. It transforms a cryptographic problem into a statistical puzzle, allowing an analyst to uncover the key by identifying and comparing patterns rather than attempting to guess every possible key (brute-force).

While modern encryption algorithms are specifically designed to resist frequency analysis by creating a uniform, random-looking distribution, this principle remains a cornerstone of classical cryptanalysis. It serves as a crucial lesson in cybersecurity: <em>patterns are a form of information</em>, and any discernible pattern in a seemingly random system can be exploited.




---

 *Published by Ludovica*  
 *October 14, 2025*
