---
layout: post
title: "Deriving Simple Recurrence Formulas for Mean and Variance"
date: 2025-10-30 10:00:00 +0100
categories: statistics homework
---

# Deriving Simple Recurrence Formulas for Mean and Variance

When processing data streams or large datasets, recalculating the mean and variance from scratch for every new observation (Xₙ) is computationally inefficient. Instead, we can use **recurrence (or online) formulas** that update the statistics based only on the previous statistics (for n-1 observations) and the new observation. This approach simplifies the process considerably and is often essential in real-time applications.

### 1. Recursive Formula for the Mean (Xₙ)

The mean of n observations (Xₙ) is defined as:
<br>
<code>Mean(n) = (1/n) * Σ [Xᵢ]</code>
<br>
(Dove Σ indica la sommatoria da i=1 a n).

We can separate the sum into the first n-1 elements and the n<sup>th</sup> element (Xₙ):
<br>
<code>Mean(n) = (1/n) * [ Σ (i=1 to n-1) Xᵢ + Xₙ ]</code>
<br>

We know that the sum of the first n-1 elements is equal to (n-1) times the previous mean (X<sub>n-1</sub>):
<br>
<code>Σ (i=1 to n-1) Xᵢ = (n-1) * Mean(n-1)</code>
<br>

Substituting this back into the formula for Xₙ:
<br>
<code>Mean(n) = (1/n) * [ (n-1) * Mean(n-1) + Xₙ ]</code>
<br>

To simplify, we can rewrite (n-1)/n as 1 - 1/n:
<br>
<code>Mean(n) = (1 - 1/n) * Mean(n-1) + (1/n) * Xₙ</code>
<br>

Alternatively, a very simple and numerically stable recursive update can be derived by calculating the difference (or error) caused by the new observation:
<br>
<code>Mean(n) = Mean(n-1) + [ Xₙ - Mean(n-1) ] / n</code>
<br>
**This final formula is the simplest recurrence relation for the mean**. The term (Xₙ - X<sub>n-1</sub>) is the error, which is then corrected by distributing it across all n observations.

### 2. Recursive Formula for the Variance (Vₙ)

The variance is typically calculated using the sum of squared differences from the mean. Let Sₙ be the sum of squared differences for n observations:
<br>
<code>Sₙ = Σ [ (Xᵢ - Mean(n))² ]</code>
<br>
(Dove Σ indica la sommatoria da i=1 a n).

For recursive calculation, we focus on how Sₙ relates to S<sub>n-1</sub> and the new data point Xₙ. This derivation often follows **Welford’s method** to maintain numerical stability.

Let S<sub>n-1</sub> be the previous sum of squared differences. The new sum S<sub>n</sub> can be related to S<sub>n-1</sub> using the following update, which incorporates the previous mean (X<sub>n-1</sub>) and the new mean (Xₙ) [4, 5]:
<br>
<code>Sₙ = S(n-1) + (Xₙ - Mean(n-1)) * (Xₙ - Mean(n))</code>
<br>

While the term X<sub>n</sub> - X<sub>n</sub> can be complex to calculate recursively, we substitute the mean update formula (Xₙ = X<sub>n-1</sub> + δₙ / n, where δₙ = Xₙ - X<sub>n-1</sub> is the error):
<br>
<code>Sₙ = S(n-1) + δₙ * [ δₙ - (δₙ / n) ]</code>
<br>

Simplifying this leads to the **recurrence formula for the sum of squared differences**:
<br>
<code>Sₙ = S(n-1) + [ (n-1)/n ] * (Xₙ - Mean(n-1))²</code>
<br>

Once Sₙ is calculated, the sample variance Vₙ is obtained by dividing by n-1 (for n>1) [5, 6]:
<br>
<code>Vₙ = Sₙ / (n-1)</code>
<br>

### 3. Example Code Implementation

The following Python code demonstrates the use of these recurrence formulas to calculate the mean and variance sequentially as new data arrives. This implements a stable, single-pass algorithm often used in data streaming environments.


    import random
    def calculate_online_statistics(data_stream):
    
        n = 0 
        mean = 0.0 
        M2 = 0.0 # M2 is the sum of squares of differences (S_n)

        for x in data_stream: 
            n += 1 
            if n == 1: 
                mean = x 
                M2 = 0.0 
            else: 
                # 1. Update Mean (Recurrence Formula: Mean(n) = Mean(n-1) + [ Xₙ - Mean(n-1) ] / n)
                delta = x - mean 
                mean += delta / n 
                
                # 2. Update Sum of Squared Differences (Recurrence Formula for S_n)
                # This is the Welford update optimized for stability:
                delta2 = x - mean 
                M2 += delta * delta2 
        
        variance = M2 / (n - 1) if n > 1 else 0.0 
        return mean, variance, n
        
        # Example usage with random data
        data = [random.randint(1, 100) for _ in range(10)]
        final_mean, final_variance, total_count = calculate_online_statistics(data)

        print(f"Data Stream: {data}")
        print(f"Total Observations (n): {total_count}")
        print(f"Online Mean: {final_mean:.4f}")
        print(f"Online Variance: {final_variance:.4f}")

        # Compare with built-in functions (using numpy for comparison only)
        try: 
            import numpy as np 
            np_mean = np.mean(data) 
            np_variance = np.var(data, ddof=1) # ddof=1 for sample variance
            print("\n--- Verification (Numpy) ---")
            print(f"Numpy Mean: {np_mean:.4f}")
            print(f"Numpy Sample Variance: {np_variance:.4f}")
        except ImportError: 
            pass




### 4. Expanding Measures of Location: Beyond the Mean

The Arithmetic Mean (Xₙ) is the most common measure of central tendency, and its recursive calculation is straightforward, as demonstrated above. However, other **measures of location** (or averages) have been proposed, each useful under specific data circumstances. Using the appropriate measure is critical, especially in cybersecurity where data often involves outliers or non-normal distributions.

The goal remains to calculate these measures **"online,"** recalculating the statistic efficiently every time a new value (Xₙ) is added, thereby avoiding the need to store the entire dataset.

#### 4.1 The Median

The Median (M) is the middle value of a dataset, separating the higher half from the lower half.

**When is it Useful?**
The Median is highly **robust against outliers** and is preferred for **skewed distributions** (like network latency or financial income) where the Mean would be pulled excessively high or low by extreme values.

**Online Calculation:**
Calculating the Median recursively is significantly more complex than the Mean. Since the Median depends on the rank and sort order of all data points, a simple formula based only on the previous median (M<sub>n-1</sub>) and the new point (Xₙ) is **not possible**.
Instead, online median calculation typically relies on data structures like **two balanced heaps (min-heap and max-heap)** or specialized data sketches (e.g., T-Digest or Q-Digest) to efficiently track the middle elements as new data arrives.

#### 4.2 The Mode

The Mode is the value that appears most frequently in a dataset.

**When is it Useful?**
The Mode is essential for **categorical data** (e.g., protocol types, error codes) or discrete numerical data where identifying the most common event is necessary. In cybersecurity, this could identify the most frequent type of attempted attack or the most common user behavior.

**Online Calculation:**
The Mode is the simplest measure of location to calculate recursively. The update relies on tracking the frequency count of each observed value (X<sub>i</sub>) and the maximum frequency observed so far.

**Online Formula Concept (Frequency Counter):**

Let F(X<sub>i</sub>) be the frequency count of value X<sub>i</sub>, and M be the current Mode.

1.  **Update Frequency:** When a new observation Xₙ arrives, increment its count:
    F(Xₙ) = F(Xₙ) + 1
2. <strong>Update Mode (M):</strong> If the new count F(X<sub>n</sub>) exceeds the current maximum frequency (or the frequency of the current Mode F(M)), then X<sub>n</sub> becomes the new Mode:
   <br><em>If</em>         F(X<sub>n</sub>) &gt; F(M), <em>then</em> M = X<sub>n</sub>


This requires an efficient way to store and retrieve counts, such as a hash map or dictionary, which remains scalable for large streams of data.





### 5. Computational Advantage of Online Algorithms

The adoption of **recurrence (or online) formulas** is not merely an academic exercise; it provides significant computational advantages, especially when dealing with data streams or **large datasets**.

In essence, online algorithms transform the computational complexity of calculating statistics from a dependence on the size of the entire history (N) to a constant time per update.

### Efficiency Comparison

Traditional (Batch) calculation of statistics, such as the Mean(X<sub>n</sub>), requires summing all n data points ∑(from i=1 to n)X<sub>i</sub>. If a new data point arrives, the entire process must be repeated.

*   **Batch Complexity:** When a new observation is added, recalculating the mean requires an operation proportional to the total number of observations, O(N). For N updates, the total complexity grows as O(N<sup>2</sup>).
*   **Online Complexity:** Recurrence formulas are designed to update the statistics based only on the **previous statistics (for n-1 observations) and the new observation** (X<sub>n</sub>).

The recursive Mean formula, for instance:
X<sub>n</sub> = X<sub>n-1</sub> + (X<sub>n</sub> - X<sub>n-1</sub>) / n
requires only a few arithmetic operations (subtraction, division, addition). This means that the computational cost per update is constant, O(1).

### Key Computational Advantages

The computational benefits of using these single-pass algorithms are summarized as follows:

1.  **Efficiency and Speed:** Online methods ensure that recalculating the mean and variance for every new observation (X<sub>n</sub>) is **not computationally inefficient**. The constant time complexity O(1) per update is crucial for applications that require fast, **real-time applications** and processing high-velocity data streams.
2.  **Memory Management:** Since these algorithms only require the storage of the current statistics (e.g., n, X<sub>n-1</sub>, and S<sub>n-1</sub> for variance), they eliminate the need to **store the entire dataset**. This makes them indispensable for **large datasets** or environments with limited memory resources.
3.  **Numerical Stability:** Methods like Welford’s method, used for the recursive calculation of the variance, are specifically designed to **maintain numerical stability**. This is critical as, in traditional batch calculations, summing many small squared differences can lead to significant floating-point precision errors, especially in large datasets.

In summary, online algorithms provide a mechanism to **simplify the process considerably** by ensuring that the computational effort remains minimal and constant, regardless of the overall size of the data stream.s